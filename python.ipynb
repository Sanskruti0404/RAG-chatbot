{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz  # PyMuPDF\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    document = fitz.open(pdf_path)\n",
    "    text = \"\"\n",
    "    for page_num in range(len(document)):\n",
    "        page = document.load_page(page_num)\n",
    "        text += page.get_text()\n",
    "    return text\n",
    "\n",
    "pdf_text = extract_text_from_pdf(\"policy-booklet-0923.pdf\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = {\n",
    "    \"query\": [\n",
    "        \"What is the definition of 'insured vehicle'?\",\n",
    "        \"What does the policy cover for accidental damage?\",\n",
    "        \"Are accessories covered under the policy?\",\n",
    "        \"Does the policy cover personal belongings?\",\n",
    "        \"What is the excess for accidental damage?\",\n",
    "        \"What happens if I drive another car?\",\n",
    "        \"Is windscreen damage covered?\",\n",
    "        \"What is the procedure to make a claim?\",\n",
    "        \"Are courtesy cars provided during repairs?\",\n",
    "        \"What is the policy duration?\",\n",
    "        \"How can I cancel my policy?\",\n",
    "        \"What is the limit for medical expenses cover?\",\n",
    "        \"Does the policy cover damage from natural disasters?\",\n",
    "        \"What is covered under third-party liability?\",\n",
    "        \"Is there any cover for legal expenses?\",\n",
    "        \"What are the conditions for a no-claims discount?\",\n",
    "        \"Can I drive in the EU with this policy?\",\n",
    "        \"What is covered under breakdown assistance?\",\n",
    "        \"Are there any exclusions to the policy?\",\n",
    "        \"What is the process for adding a driver to the policy?\",\n",
    "        \"What happens if my car is stolen?\",\n",
    "        \"Does the policy cover modifications to the vehicle?\",\n",
    "        \"What is the limit for personal accident cover?\",\n",
    "        \"How do I report an accident?\",\n",
    "        \"What are the benefits of using an approved repairer?\",\n",
    "        \"Is there a mileage limit for this policy?\",\n",
    "        \"What types of vehicles are covered?\",\n",
    "        \"What is the grace period for policy renewal?\",\n",
    "        \"Are trailers covered under this policy?\",\n",
    "        \"What should I do if my personal details change?\"\n",
    "    ],\n",
    "    \"response\": [\n",
    "        \"The vehicle described in the current certificate of motor insurance.\",\n",
    "        \"We will pay for damage to the insured vehicle caused accidentally.\",\n",
    "        \"Yes, accessories are covered as long as they are in or on the vehicle.\",\n",
    "        \"Yes, we cover personal belongings up to £250.\",\n",
    "        \"The excess for accidental damage is £100.\",\n",
    "        \"You are covered third party only if the other car is not owned by you or hired to you.\",\n",
    "        \"Yes, windscreen damage is covered with an excess of £75.\",\n",
    "        \"To make a claim, call our 24-hour claims line on the number provided in your policy document.\",\n",
    "        \"Yes, a courtesy car is provided if your car is being repaired by one of our approved repairers.\",\n",
    "        \"The policy is effective for one year from the start date shown in the certificate of insurance.\",\n",
    "        \"You can cancel your policy by providing us with written notice.\",\n",
    "        \"We will pay up to £100 for each person for medical expenses incurred.\",\n",
    "        \"Yes, the policy covers damage caused by natural disasters such as floods or storms.\",\n",
    "        \"The policy covers your legal liability for death or injury to others and damage to their property.\",\n",
    "        \"Legal expenses cover is not included as standard but can be added as an optional extra.\",\n",
    "        \"A no-claims discount is applied if you do not make any claims during the policy year.\",\n",
    "        \"Yes, the policy provides the minimum legal cover required to drive in the EU.\",\n",
    "        \"Breakdown assistance includes roadside repair and recovery to the nearest garage.\",\n",
    "        \"Yes, exclusions include driving under the influence of alcohol or drugs, and using the vehicle for racing.\",\n",
    "        \"To add a driver, you must contact us with their details, and an additional premium may be required.\",\n",
    "        \"If your car is stolen, we will pay for its replacement or the market value of the vehicle.\",\n",
    "        \"Modifications are covered only if they are declared and accepted by us.\",\n",
    "        \"We provide up to £5,000 for death or serious injury caused by a car accident.\",\n",
    "        \"Report the accident by calling our claims line and providing all necessary details.\",\n",
    "        \"Using an approved repairer ensures quality repairs and a courtesy car while your vehicle is being fixed.\",\n",
    "        \"No, there is no mileage limit imposed on the policy.\",\n",
    "        \"The policy covers private cars used for social, domestic, and pleasure purposes.\",\n",
    "        \"There is no formal grace period; the policy must be renewed before the expiry date.\",\n",
    "        \"Trailers are covered while attached to the insured vehicle, but there is no cover for their contents.\",\n",
    "        \"Notify us immediately if any of your personal details change to ensure your policy remains valid.\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df.to_csv(\"queries_responses.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RagTokenizer, RagRetriever, RagSequenceForGeneration\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv(r\"C:\\Users\\Sanskruti\\Desktop\\Workspace2\\queries_responses.csv\")\n",
    "dataset = Dataset.from_pandas(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Sanskruti\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Load the dataset\n",
    "dataset_df = pd.read_csv(r\"C:\\Users\\Sanskruti\\Desktop\\Workspace2\\queries_responses.csv\")\n",
    "\n",
    "# Load a pre-trained Sentence Transformer model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Encode the text data\n",
    "embeddings = model.encode(dataset_df['response'].tolist(), convert_to_tensor=True)\n",
    "\n",
    "# Create FAISS index\n",
    "index = faiss.IndexFlatL2(embeddings.shape[1])\n",
    "index.add(embeddings.cpu().detach().numpy())\n",
    "\n",
    "# Save the FAISS index\n",
    "faiss.write_index(index, \"faiss_index.bin\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Sanskruti\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS index saved as faiss_index.bin\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Load the dataset\n",
    "dataset_df = pd.read_csv(r\"C:\\Users\\Sanskruti\\Desktop\\Workspace2\\queries_responses.csv\")\n",
    "\n",
    "# Load a pre-trained Sentence Transformer model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Encode the responses\n",
    "embeddings = model.encode(dataset_df['response'].tolist(), convert_to_tensor=True)\n",
    "\n",
    "# Convert embeddings to numpy array\n",
    "embeddings_np = embeddings.cpu().detach().numpy()\n",
    "\n",
    "# Create FAISS index\n",
    "dimension = embeddings_np.shape[1]\n",
    "index = faiss.IndexFlatL2(dimension)\n",
    "index.add(embeddings_np)\n",
    "\n",
    "# Save the FAISS index\n",
    "faiss.write_index(index, \"faiss_index.bin\")\n",
    "\n",
    "print(\"FAISS index saved as faiss_index.bin\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3d97dd178b441f08db9702dbf962ad2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/30 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset saved in Hugging Face format\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "\n",
    "# Load the dataset using Hugging Face's datasets library\n",
    "dataset = Dataset.from_pandas(dataset_df)\n",
    "\n",
    "# Save the dataset in the appropriate format for RAG\n",
    "dataset.save_to_disk(\"query_response_dataset\")\n",
    "\n",
    "print(\"Dataset saved in Hugging Face format\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "\n",
    "# Load your CSV\n",
    "df = pd.read_csv(\"queries_responses.csv\")\n",
    "\n",
    "# Create the required columns\n",
    "df = df.rename(columns={\"query\": \"title\", \"response\": \"text\"})\n",
    "df[\"embeddings\"] = None  # Placeholder for embeddings\n",
    "\n",
    "# Convert to Huggingface Dataset\n",
    "dataset = Dataset.from_pandas(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Sanskruti\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Load your CSV\n",
    "df = pd.read_csv(r\"C:\\Users\\Sanskruti\\Desktop\\Workspace2\\queries_responses.csv\")\n",
    "\n",
    "# Create the required columns\n",
    "df = df.rename(columns={\"query\": \"title\", \"response\": \"text\"})\n",
    "df[\"embeddings\"] = None  # Placeholder for embeddings\n",
    "\n",
    "# Convert to Huggingface Dataset\n",
    "dataset = Dataset.from_pandas(df)\n",
    "\n",
    "# Load a pre-trained sentence transformer model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Generate embeddings for the text column\n",
    "embeddings = model.encode(df['text'].tolist(), convert_to_numpy=True)\n",
    "\n",
    "# Assign embeddings back to the dataframe\n",
    "df[\"embeddings\"] = list(embeddings)  # Convert numpy arrays to lists\n",
    "\n",
    "# Convert back to Huggingface Dataset\n",
    "dataset = Dataset.from_pandas(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2f7525e27a54b49808d21c749b5a851",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/30 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "# Save the dataset to disk\n",
    "dataset_path = \"./dataset\"\n",
    "index_path = \"./index\"\n",
    "dataset.save_to_disk(dataset_path)\n",
    "\n",
    "# Save the FAISS index\n",
    "import faiss\n",
    "\n",
    "# Ensure embeddings are a 2D numpy array\n",
    "embeddings_2d = np.vstack(dataset[\"embeddings\"]) # type: ignore\n",
    "\n",
    "# Create and train the FAISS index\n",
    "faiss_index = faiss.IndexFlatL2(embeddings_2d.shape[1])\n",
    "faiss_index.add(embeddings_2d)\n",
    "\n",
    "# Save the FAISS index to disk\n",
    "faiss.write_index(faiss_index, index_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "# Step 1: Load and Prepare Data\n",
    "def load_data(file_path):\n",
    "    data = pd.read_csv(r\"C:\\Users\\Sanskruti\\Desktop\\Workspace2\\queries_responses.csv\")\n",
    "    return data\n",
    "\n",
    "# Step 2: Train Model\n",
    "def train_model(data):\n",
    "    model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    embeddings = model.encode(data['response'].tolist(), convert_to_tensor=True)\n",
    "    return model, embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Train Model\n",
    "def train_model(data):\n",
    "    model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    embeddings = model.encode(data['response'].tolist(), convert_to_tensor=True)\n",
    "    return model, embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Create FAISS Index\n",
    "def create_faiss_index(embeddings):\n",
    "    embeddings_np = embeddings.cpu().detach().numpy()\n",
    "    dimension = embeddings_np.shape[1]\n",
    "    index = faiss.IndexFlatL2(dimension)\n",
    "    index.add(embeddings_np)\n",
    "    faiss.write_index(index, \"faiss_index.bin\")\n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Query Handling\n",
    "def handle_query(query, model, index, data):\n",
    "    query_embedding = model.encode([query], convert_to_tensor=True)\n",
    "    query_embedding_np = query_embedding.cpu().detach().numpy()\n",
    "    D, I = index.search(query_embedding_np, k=1)\n",
    "    return data['response'].iloc[I[0][0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Evaluation and Improvement\n",
    "def evaluate_model(test_data, model, index, data):\n",
    "    correct = 0\n",
    "    for _, row in test_data.iterrows():\n",
    "        response = handle_query(row['query'], model, index, data)\n",
    "        if response == row['response']:\n",
    "            correct += 1\n",
    "    accuracy = correct / len(test_data)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Sanskruti\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: The vehicle described in the current certificate of motor insurance.\n"
     ]
    }
   ],
   "source": [
    "# Main script to execute the steps\n",
    "data = load_data('queries_responses.csv')\n",
    "model, embeddings = train_model(data)\n",
    "index = create_faiss_index(embeddings)\n",
    "\n",
    "# Example of handling a query\n",
    "query = \"What is the definition of 'insured vehicle'?\"\n",
    "response = handle_query(query, model, index, data)\n",
    "print(f\"Response: {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy: 0.77\n"
     ]
    }
   ],
   "source": [
    "# Evaluate model\n",
    "test_data = load_data('test_queries_responses.csv')\n",
    "accuracy = evaluate_model(test_data, model, index, data)\n",
    "print(f\"Model Accuracy: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Sanskruti\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f991ac44549144398cb25fc181dd5830",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/30 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "# Load your CSV using a raw string for the file path\n",
    "df = pd.read_csv(r\"C:\\Users\\Sanskruti\\Desktop\\Workspace2\\queries_responses.csv\")\n",
    "\n",
    "# Create the required columns\n",
    "df = df.rename(columns={\"query\": \"title\", \"response\": \"text\"})\n",
    "df[\"embeddings\"] = None  # Placeholder for embeddings\n",
    "\n",
    "# Convert to Huggingface Dataset\n",
    "dataset = Dataset.from_pandas(df)\n",
    "\n",
    "# Load a pre-trained sentence transformer model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Generate embeddings for the text column\n",
    "embeddings = model.encode(df['text'].tolist(), convert_to_numpy=True)\n",
    "\n",
    "# Assign embeddings back to the dataframe\n",
    "df[\"embeddings\"] = list(embeddings)  # Convert numpy arrays to lists\n",
    "\n",
    "# Convert back to Huggingface Dataset\n",
    "dataset = Dataset.from_pandas(df)\n",
    "\n",
    "# Save the dataset to disk\n",
    "dataset.save_to_disk(\"./dataset\")\n",
    "\n",
    "# Ensure embeddings are a 2D numpy array\n",
    "embeddings_2d = np.vstack(df[\"embeddings\"])\n",
    "\n",
    "# Create and train the FAISS index\n",
    "faiss_index = faiss.IndexFlatL2(embeddings_2d.shape[1])\n",
    "faiss_index.add(embeddings_2d)\n",
    "\n",
    "# Save the FAISS index to disk\n",
    "faiss.write_index(faiss_index, \"./index\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
